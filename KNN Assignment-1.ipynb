{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d676aa-d6e4-4375-8dce-8b6468b702cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER1. \n",
    "KNN, or k-Nearest Neighbors, is a supervised machine learning algorithm used for both classification and regression tasks. It classifies a new data point based on the majority class or averages the values of its k-nearest neighbors in the feature space.\n",
    "\n",
    "ANSWER2. \n",
    "Choosing the value of K in KNN involves balancing bias and variance. A smaller K value makes the model sensitive to noise, while a larger K may lead to oversmoothing. Cross-validation and grid search are common methods to find an optimal K.\n",
    "\n",
    "ANSWER3. \n",
    "KNN classifier is used for classification tasks, assigning the class label based on the majority class of the k-nearest neighbors. KNN regressor, on the other hand, is used for regression tasks, predicting the output as the average of the target values of its k-nearest neighbors.\n",
    "\n",
    "ANSWER4. \n",
    "Performance in KNN is measured using metrics like accuracy, precision, recall, F1 score (for classification), and mean squared error or R-squared (for regression).\n",
    "\n",
    "ANSWER5. \n",
    "The curse of dimensionality in KNN refers to the deterioration of algorithm performance as the number of features increases. This leads to increased computational complexity and a sparsity of data in high-dimensional spaces.\n",
    "\n",
    "ANSWER6. \n",
    "Handling missing values in KNN involves imputing them based on the values of their neighbors. The missing feature is estimated using the average or weighted average of the neighboring points.\n",
    "\n",
    "ANSWER7. \n",
    "The choice between KNN classifier and regressor depends on the nature of the problem. For categorical outcomes, a classifier is suitable, while for continuous outcomes, a regressor is appropriate.\n",
    "\n",
    "ANSWER8. \n",
    "Strengths of KNN include simplicity, effectiveness in low-dimensional spaces, and no need for training. Weaknesses include sensitivity to irrelevant features, high computational cost in high-dimensional spaces, and the need for feature scaling. Addressing these involves feature selection, dimensionality reduction, and normalization.\n",
    "\n",
    "ANSWER9. \n",
    "Euclidean distance is the straight-line distance between two points, while Manhattan distance is the sum of the absolute differences between corresponding coordinates. In KNN, both can be used as distance metrics to measure proximity.\n",
    "\n",
    "ANSWER10. \n",
    "Feature scaling is crucial in KNN because it ensures that all features contribute equally to the distance calculation. Without scaling, features with larger scales might dominate the distance metric, leading to biased results. Common scaling methods include min-max scaling and z-score normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
